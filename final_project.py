# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zACJ7QXpV7oljQnNM8mn1xu_W9hiQvXb

## Using the Yelp API to Investigate the Restaurant Scene in Los Angeles County
"""

import requests
import pandas as pd
from pandas import json_normalize
import json
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import string
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity, cosine_distances
import numpy as np
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.preprocessing import StandardScaler , Normalizer, MinMaxScaler, OneHotEncoder, RobustScaler, MaxAbsScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

api_key = '3F_lCJ4vg0r62oOgRHcDXSxsqjj2mVGGu-kgdXiBPTwHynizbHrphlZaZ0pYEuxNtcZw86gdnBVr7J0wVuZumktYiTZzhFEWJ-rlsS4anGkqcTnSL_tJ9NYRyTxIYHYx'
headers = {'Authorization': 'Bearer %s' % api_key}

"""### Collect and clean data

Create dataset for > 1000 restaurants in LA County 
API Link: https://api.yelp.com/v3/businesses/search
"""

locations = ['Los Angeles', 'Palmdale', 'Lancaster', 'Santa Clarita', 'Long Beach', 'Glendale', 'Pasadena', 'Pomona',
             'Torrance', 'Malibu', 'Hawaiian Gardens', 'Gardena', 'Hermosa Beach', 'Maywoord', 'Cudahy', 'Artesia', 
             'Hidden Hills', 'West Hollywood', 'Lomita', 'Bradbury', 'Lawndale', 'San Gabriel', 'El Segundo', 'Culver City',
             'Huntington Park', 'Lawndale', 'Signal Hill', 'Downey']
df_restaurants = pd.DataFrame()
business_search_url = 'https://api.yelp.com/v3/businesses/search'
for loc in locations:
  param={'term':'restaurants','location': loc, 'limit': 50}
  response=requests.get(business_search_url, headers=headers, params=param)
  temp = json_normalize(response.json()['businesses'])
  df_restaurants = pd.concat([df_restaurants, temp])

df = df_restaurants

df.to_csv('restaurants.csv')

#import file restaurants.csv 
df_restaurants = pd.read_csv("restaurants.csv")
df_restaurants = df_restaurants[df_restaurants.columns[1:]]
df_restaurants = df_restaurants[['id', 'name','review_count','categories','rating', 'price','display_phone','coordinates.latitude','coordinates.longitude', 'location.address1','location.address2','location.city','location.zip_code','location.state','location.country']]
df_restaurants.head()

"""Reviews dataset contains: review_id, user_id, business_id, 
review, ratings, date

API Link: https://api.yelp.com/v3/businesses/{id}/reviews

"""

df_reviews = pd.DataFrame()
for id in df_restaurants['id']:
  url = 'https://api.yelp.com/v3/businesses/%s/reviews' % str(id)
  response=requests.get(url, headers=headers)
  if (response.status_code == 200):
    temp = json_normalize(response.json()['reviews'])
    temp['business_id'] = id
    df_reviews = pd.concat([df_reviews, temp])

# df_reviews = pd.read_csv("reviews.csv")
df_reviews.head()

"""### Part 1: Overview of restaurants landscape in Los Angeles County 

"""

df_restaurants["location.city"].value_counts()

#number of restaurants per city, 10 most popular
city = df_restaurants["location.city"].value_counts()
top10 = city.head(10)
top10.plot.bar()
plt.title("Top 10 Most Restaurants Per City")
plt.xlabel("Cities")
plt.ylabel("Number of Restaurants With Reviews")

#average yelp rating in each city
df_top10 = df_restaurants[(df_restaurants["location.city"] == "Los Angeles") | (df_restaurants["location.city"] == "Long Beach") | (df_restaurants["location.city"] == "Gardena") |
(df_restaurants["location.city"] == "Pasadena") | (df_restaurants["location.city"] == "Glendale") | (df_restaurants["location.city"] == "Downey") | (df_restaurants["location.city"] == "Lancaster") |
(df_restaurants["location.city"] == "Torrance") | (df_restaurants["location.city"] == "Malibu") | (df_restaurants["location.city"] == "hermosa Beach")]
(df_top10.groupby("location.city")["rating"].mean().plot.bar())
plt.title("Average Yelp Rating per City")
plt.xlabel("Cities")
plt.ylabel("Average Rating")

df_top10.groupby("location.city")["rating"].mean()

#average yelp rating per type of restaurant
df_restaurants["type"] = df_restaurants["categories"]
df_restaurants["type"] = df_restaurants["type"].astype(str)
df_restaurants["type"] = df_restaurants["type"].str.replace('[', '')
df_restaurants["type"] = df_restaurants["type"].str.replace(']', '')
df_restaurants["type"] = df_restaurants["type"].str.replace('{', '')
df_restaurants["type"] = df_restaurants["type"].str.replace('}', '')
df_restaurants["type"] = df_restaurants["type"].str.replace('\'', '')
df_restaurants["type"] = df_restaurants["type"].str.replace("alias:", '')
df_restaurants["type"] = df_restaurants["type"].str.replace("title:", '')
df_restaurants["type"] = df_restaurants["type"].str.split(",")


for i in range(0,len(df_restaurants["type"])-1):
  df_restaurants["type"][i] = df_restaurants["type"][i][0]

df_restaurants

types = df_restaurants["type"].value_counts()
top10 = types.head(10)
top10.plot.bar()
plt.title("Top 10 Most Common Types of Restaurants")
plt.xlabel("Restaurant Type")
plt.ylabel("Number of Restaurants in LA")

#review counts vs rating
plt.scatter(df_restaurants['review_count'],df_restaurants['rating'])
plt.title("Review Count vs Rating Score")
plt.xlabel("Review Count")
plt.ylabel("Rating Score")
plt.show()

from altair import *

Chart(df_restaurants).mark_circle().encode(
    x="review_count",
    y="rating",
    column = "price"
)

df_restaurants.groupby("price")["rating"].mean()

"""### Part 2: Fitting a Model to Predict a Restaurant's Rating"""

df_restaurants_new = df_restaurants[['review_count', 'price', 'location.city', 'location.zip_code', 'type', 'rating']].copy()
df_restaurants_new = df_restaurants_new.dropna()
df_restaurants_new['location.zip_code'] = df_restaurants_new['location.zip_code'].astype(str)
df_restaurants_new['location.city'] = df_restaurants_new['location.city'].astype(str)
df_restaurants_new['price'] = df_restaurants_new['price'].astype(str)
df_restaurants_new['rating'] = df_restaurants_new['rating'].astype(int)
df_restaurants_new['type'] = df_restaurants_new['type'].astype(str)
df_restaurants_new['review_count'] = df_restaurants_new['review_count'].astype(int)
df_restaurants_new.loc[1359,'type'] = 'cuban'
df_restaurants_new

def kmodels(scaler, distance, x_train, y_train, x_test, y_test):
  ct = make_column_transformer(
    (OneHotEncoder(handle_unknown = 'ignore'), list(x_train.columns)[1:]),
    remainder = "passthrough"
  )
  ct2 = make_column_transformer(
    (RobustScaler(), ['review_count']),
    (OneHotEncoder(handle_unknown='ignore'), list(x_train.columns)[1:]),
    remainder = "passthrough"
  )
  ct3 = make_column_transformer(
    (MaxAbsScaler(), ['review_count']),
    (OneHotEncoder(handle_unknown = 'ignore'), list(x_train.columns)[1:]),
    remainder = "passthrough"
  )
  if scaler == 'standard':
    pipeline = make_pipeline(ct, StandardScaler(with_mean=False), 
                             KNeighborsRegressor(n_neighbors=20, metric=distance))
  elif scaler == 'normalized':
    pipeline = make_pipeline(ct, Normalizer(), 
                             KNeighborsRegressor(n_neighbors=20, metric=distance))
  elif scaler == 'maxabs':
    pipeline = make_pipeline(ct3, MaxAbsScaler(), 
                             KNeighborsRegressor(n_neighbors=20, metric=distance))
  elif scaler == 'robust':
    pipeline = make_pipeline(ct2, RobustScaler(with_centering=False), 
                             KNeighborsRegressor(n_neighbors=20, metric=distance))
  pipeline.fit(X=x_train, y=y_train)
  predict = pipeline.predict(X=x_test)
  rmse = np.sqrt(mean_squared_error(y_test, predict))
  pipescore = pipeline.score(x_test, y_test)

  print('rmse for {}, {}: {}'.format(scaler, distance, rmse))
  print('model accuracy score for {}, {}: {}\n'.format(scaler, distance, pipescore))
  return pipescore

#predict restaurants rating based on it's features
df_train, df_test = train_test_split(df_restaurants_new, test_size=0.2)

x_train1 = df_train[['review_count', 'price', 'location.city', 'location.zip_code', 'type']]
x_test1 = df_test[['review_count', 'price', 'location.city', 'location.zip_code', 'type']]
x_train2 = df_train[['review_count', 'price', 'location.city', 'type']]
x_test2 = df_test[['review_count', 'price', 'location.city', 'type']]
x_train3 = df_train[['review_count', 'price', 'type']]
x_test3 = df_test[['review_count', 'price', 'type']]
x_train4 = df_train[['review_count', 'price', 'location.city']]
x_test4 = df_test[['review_count', 'price','location.city']]
x_train5 = df_train[['review_count','location.city']]
x_test5 = df_test[['review_count','location.city']]
x_train6 = df_train[['review_count', 'price',]]
x_test6 = df_test[['review_count', 'price',]]
list_x_test = [x_test1,x_test2,x_test3,x_test4,x_test5,x_test6]
list_x_train = [x_train1, x_train2, x_train3, x_train4, x_train5, x_train6]
y_train = df_train['rating']
y_test = df_test['rating']

model_types = ['standard', 'normalized', 'maxabs', 'robust']
distance_types = ['euclidean', 'manhattan', 'minkowski']
df_model = pd.DataFrame()
for i in range(len(list_x_train)):
  print("___________________________________________________________")
  print("train model based on " + str(list(list_x_train[i].columns)))
  df_model.loc[i,'features'] = str(list(list_x_train[i].columns))
  print("___________________________________________________________")
  max_score = []
  for model in model_types:
    for distance in distance_types:
      max_score.append(kmodels(model, distance, list_x_train[i], y_train, list_x_test[i] , y_test))
  df_model.loc[i,'max score'] = np.max(max_score)

df_model.plot.bar(x='features',y='max score', figsize=(10, 5))
plt.title("Maximum Accuracy Score for Different Combinations of Features")

"""### Part 3: Advance the restaurant recommendation system based on user’s preferences """

def clean_data(data):
  data = data.lower()
  data = ''.join([char for char in data if char not in string.punctuation])
  data = ' '.join([word for word in data.split() if word not in stopwords.words('english')])
  return data

def search_input(search, top_n):
  search_row = pd.DataFrame({'business_id': 'None', 'user.id': 'None', 'rating': 'None', 'text': search}, index=[0])
  search = pd.concat([search_row, yelp_review])
  vec = TfidfVectorizer(norm=None)
  vec.fit(yelp_review['text'].fillna("None"))
  tf_idf_sparse = vec.transform(search['text'])
  similarity = cosine_similarity(tf_idf_sparse)[0]
  return search.iloc[similarity.argsort()[-(top_n+1):][::-1][1:]]

df_reviews = df_reviews.rename(columns ={'id' : 'review_id'})
df_reviews.head()

# df_reviews = pd.read_csv("reviews.csv")
# df_reviews = df_reviews[df_reviews.columns[1:]]
df_reviews = df_reviews[['review_id','text','rating','time_created','user.id','business_id']]
yelp_review = df_reviews[['business_id','user.id', 'rating', 'text']]
yelp_review.head()

yelp_review.loc[0,'text']

yelp_review['text'] = yelp_review['text'].apply(clean_data)
yelp_review = yelp_review.drop_duplicates()

yelp_review.loc[0,'text']

# search for a place
search = 'i want mexican food'
recommend_place = search_input(search, 5)
print(recommend_place)
df_restaurants[np.isin(df_restaurants, recommend_place['business_id']).any(axis=1)]

# search for a place
search = "i want to have dinner with a beautiful view"
recommend_place = search_input(search, 5)
print(recommend_place)
df_restaurants[np.isin(df_restaurants, recommend_place['business_id']).any(axis=1)]